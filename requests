
#爬取网页的框架

# 1. 知道自己要爬的网页
# 2. 使用requests获取该网页的一个response对象
# 3. 对该对象进行

#requests得到的response对象,可能还需要用html解析器来解析(beautifulsoup...),否则难以使用

URL = "http://www.baidu.com"

r = requests.get(URL) 

print (r.status_code)   #http请求的返回状态，200成功,404失败,等等...

print (r.text)           #http响应内容的字符串格式,即url中的内容

print (r.content)       #.............二进制格式.............

print (r.encoding)      #从header中猜测可能的编码(utf-8...)
                        #也保存的是r的编码

print (r.apparent_encoding)       #从内容中猜测可能的编码

#requests的异常...(用的时候再查)

r.raise_for_status()    #如果不是200，则返回一个异常


requests.request()      #构造一个请求，能够使用下列方法
                        #requests.request(method,url,**kwargs)
                        #requests.request('GET',url,**kwargs)
                        
                        
#**kwargs 中的  params

kv = {'key1':'value1','key2':'value2'}
r = requests.request('GET','http://www.baidu.com',kv)
print (r.url)
# http://www.baidu.com?key1=value1&key2=value2
#html网页中，可能我们需要的数据部分储存在某个<>中，通过这个来指定抓取


#**kwargs 中的  headers

headers = {'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64)"
            " AppleWebKit/537.1 (KHTML, like Gecko) "
            "Chrome/22.0.1207.1 Safari/537.1"}
            #浏览器请求头（大部分网站没有这个请求头会报错)
            #模拟浏览器版本
            
r = requests.request('GET','http://www.baidu.com',headers = headers)

#**kwargs 中的  proxies

pxs = {'http':'http://user:pass@10.10.10.1:1234'
        'https':'https://10.10.1:4321'  }  #举个例子
        
r = requests.request('GET','http://www.baidu.com',proxies = pxs)

#即使用代理进行网页访问





requests.get()          #获取网页

requests.head()         #获取网页头信息
print(r.headers)        #头信息储存的地方

requests.post()         #向网页提交post请求

requests.put()          #向网页提交put请求

requests.patch()        #向网页提交局部修改请求

requests.delete()       #向网页提交删除请求









